#version 450 core

#include "globalubos.glsl"
#define LIGHTCACHEMODE LIGHTCACHEMODE_CREATE
#include "lightcache.glsl"
#include "gbuffer.glsl"
#include "utils.glsl"

layout(binding = 0, r32ui) restrict coherent uniform uimage3D VoxelAddressVolume;

#define LOCAL_SIZE 16

shared int cacheList[LOCAL_SIZE][LOCAL_SIZE];
shared int cacheList2[LOCAL_SIZE][LOCAL_SIZE]; // Second cache list for transitions

// Quasi constants, derived from AddressVolumeResolution at the start of the shader.
int AddressVolumeResolutionSq;
int AddressVolumeResolutionCubic;

int GetCache1DCoord(vec3 worldPosition, int addressVolumeCascade)
{
	ivec3 addressGridPos = clamp(ivec3((worldPosition - AddressVolumeCascades[addressVolumeCascade].Min) / AddressVolumeCascades[addressVolumeCascade].WorldVoxelSize),
									ivec3(0), ivec3(AddressVolumeResolution-1));

	int cacheLookup = addressGridPos.x + 
						addressGridPos.y * AddressVolumeResolution + 
						addressGridPos.z * AddressVolumeResolutionSq + 
						addressVolumeCascade * AddressVolumeResolutionCubic;
	return cacheLookup;
}

void AllocateCaches(int compactCacheCoordBase, int addressVolumeCascade)
{
	const ivec3 offsets[8] =
	{
		ivec3(0,0,0),
		ivec3(0,1,0),
		ivec3(0,0,1),
		ivec3(0,1,1),
		ivec3(1,0,0),
		ivec3(1,1,0),
		ivec3(1,0,1),
		ivec3(1,1,1)
	};

	compactCacheCoordBase -= AddressVolumeResolutionCubic * addressVolumeCascade;

	ivec3 base_unpackedAddressCoord_CascadeLocal;
	base_unpackedAddressCoord_CascadeLocal.z = compactCacheCoordBase / AddressVolumeResolutionSq;
	base_unpackedAddressCoord_CascadeLocal.y = (compactCacheCoordBase - base_unpackedAddressCoord_CascadeLocal.z * AddressVolumeResolutionSq) / AddressVolumeResolution;
	base_unpackedAddressCoord_CascadeLocal.x = compactCacheCoordBase % AddressVolumeResolution;

	for(int i=0; i<8; ++i)
	{
		ivec3 unpackedAddressCoord_CascadeLocal = base_unpackedAddressCoord_CascadeLocal + offsets[i % 8];
		ivec3 unpackedAddressCoord = unpackedAddressCoord_CascadeLocal;
		unpackedAddressCoord.x += addressVolumeCascade * AddressVolumeResolution;

		// Try lock.
		uint oldAddressValue = imageAtomicCompSwap(VoxelAddressVolume, unpackedAddressCoord, uint(0), uint(0xFFFFFFFF));
		if(oldAddressValue == 0) // Lock successful?
		{
			uint lightCacheIndex = atomicAdd(TotalLightCacheCount, 1);

			LightCacheEntries[lightCacheIndex].Position = unpackedAddressCoord_CascadeLocal * AddressVolumeCascades[addressVolumeCascade].WorldVoxelSize + AddressVolumeCascades[addressVolumeCascade].Min;
			//LightCacheEntries[lightCacheIndex].Cascade = addressVolumeCascade;

			// No need for atomic now since we keep it locked up
			imageStore(VoxelAddressVolume, unpackedAddressCoord, uvec4(lightCacheIndex + 1)); // +1 since zero means "cleared"
		}
	}
}

layout (local_size_x = LOCAL_SIZE, local_size_y = LOCAL_SIZE, local_size_z = 1) in;
void main()
{
	AddressVolumeResolutionSq = AddressVolumeResolution * AddressVolumeResolution;
	AddressVolumeResolutionCubic = AddressVolumeResolutionSq * AddressVolumeResolution;

	// Prepare shared memory
	cacheList[gl_LocalInvocationID.x][gl_LocalInvocationID.y] = 0xFFFFFFFF;
#ifdef ADDRESSVOL_CASCADE_TRANSITIONS
	cacheList2[gl_LocalInvocationID.x][gl_LocalInvocationID.y] = 0xFFFFFFFF;
#endif

	barrier();

	int ownCacheCoord = 0xFFFFFFFF;
	int ownCacheCoord2 = 0xFFFFFFFF;
	int addressVolumeCascade = 0xFFFFFFFF;

	if(!any(greaterThanEqual(gl_GlobalInvocationID.xy, BackbufferResolution)))
	{
		vec2 pixelPosition = vec2(gl_GlobalInvocationID.xy) + vec2(0.5);
		float depthBufferDepth = textureLod(GBuffer_Depth, pixelPosition / BackbufferResolution, 0).r;
		if(depthBufferDepth > 0.0001)
		{
			vec2 screenCor = pixelPosition / BackbufferResolution * 2.0 - vec2(1.0); 
			vec4 worldPosition4D = vec4(screenCor, depthBufferDepth, 1.0) * InverseViewProjection;
			vec3 worldPosition = worldPosition4D.xyz / worldPosition4D.w;

			// Register caches for given cascade.
			addressVolumeCascade = ComputeAddressVolumeCascade(worldPosition);
			ownCacheCoord = GetCache1DCoord(worldPosition, addressVolumeCascade);
			cacheList[gl_LocalInvocationID.x][gl_LocalInvocationID.y] = ownCacheCoord;

			// Additional register pass for smooth cascade transitions?
		#ifdef ADDRESSVOL_CASCADE_TRANSITIONS
			float cascadeTransition = ComputeAddressVolumeCascadeTransition(worldPosition, addressVolumeCascade);
			if(cascadeTransition > 0.0 && addressVolumeCascade < NumAddressVolumeCascades-1)
			{
				ownCacheCoord2 = GetCache1DCoord(worldPosition, addressVolumeCascade+1);
				cacheList2[gl_LocalInvocationID.x][gl_LocalInvocationID.y] = ownCacheCoord2;
			}
		#endif
		}
	}

	// All done with registering.
	barrier();

	// Has any neighbor that is more top down the same?
	ivec2 lookUpThread = max(ivec2(0), ivec2(gl_LocalInvocationID.xy) - ivec2(1));
	if(((cacheList[gl_LocalInvocationID.x][lookUpThread.y] != ownCacheCoord && 
		 cacheList[lookUpThread.x][gl_LocalInvocationID.y] != ownCacheCoord &&
		 cacheList[lookUpThread.x][lookUpThread.y] != ownCacheCoord) ||
		 lookUpThread == gl_LocalInvocationID.xy) &&
		 ownCacheCoord != 0xFFFFFFFF)
	{
		AllocateCaches(ownCacheCoord, addressVolumeCascade);
	}

	// Lookup in the other direction - so its likely that another warp will handle this allocation.
#ifdef ADDRESSVOL_CASCADE_TRANSITIONS
	lookUpThread = min(ivec2(LOCAL_SIZE-1), ivec2(gl_LocalInvocationID.xy) + ivec2(1));
	if(((cacheList2[gl_LocalInvocationID.x][lookUpThread.y] != ownCacheCoord2 && 
		 cacheList2[lookUpThread.x][gl_LocalInvocationID.y] != ownCacheCoord2 &&
		 cacheList2[lookUpThread.x][lookUpThread.y] != ownCacheCoord2) ||
		 lookUpThread == ivec2(LOCAL_SIZE-1)) &&
		 ownCacheCoord2 != 0xFFFFFFFF)
	{
		AllocateCaches(ownCacheCoord2, addressVolumeCascade+1);
	}
#endif
}